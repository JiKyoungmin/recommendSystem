import pandas as pd
import numpy as np
import pickle
import logging
import time
import os
from collections import defaultdict
from surprise import SVDpp, Dataset, Reader
from surprise.model_selection import GridSearchCV, train_test_split, cross_validate, KFold
from surprise import accuracy
import random

RANDOM_SEED = 662
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SVDppRecommendationPipeline:
    """
    SVD++ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸
    í•œë¼ëª¨ì•„ í”„ë¡œì íŠ¸ìš© ì‹ë‹¹ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
    ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ SVD++ ëª¨ë¸ í•™ìŠµ ë° ì¶”ì²œ ìˆ˜í–‰
    """
    
    def __init__(self, result_path):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            result_path: ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ” í´ë” ê²½ë¡œ
        """
        self.result_path = result_path
        self.processed_data = None
        self.surprise_data = None
        self.restaurant_mappings = None
        self.best_model = None
        self.best_params = None
        self.prediction_matrix = None
        
    def load_preprocessed_data(self):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        preprocessing íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ëœ result í´ë”ì˜ íŒŒì¼ë“¤ì„ ë¡œë“œ
        """
        logger.info("ğŸ“ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        # 1. CSV ë°ì´í„° ë¡œë“œ
        csv_path = os.path.join(self.result_path, 'svd_data.csv')
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        self.processed_data = pd.read_csv(csv_path)
        logger.info(f"CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.processed_data.shape}")
        
        # 2. ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
        mapping_path = os.path.join(self.result_path, 'restaurant_mappings.pkl')
        if not os.path.exists(mapping_path):
            raise FileNotFoundError(f"ë§¤í•‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mapping_path}")
        
        with open(mapping_path, 'rb') as f:
            self.restaurant_mappings = pickle.load(f)
        logger.info(f"ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ: {len(self.restaurant_mappings['restaurant_to_id'])}ê°œ ì‹ë‹¹")
        
        # 3. Surprise ë°ì´í„°ì…‹ ë¡œë“œ (ìˆìœ¼ë©´)
        dataset_path = os.path.join(self.result_path, 'surprise_dataset.pkl')
        if os.path.exists(dataset_path):
            with open(dataset_path, 'rb') as f:
                self.surprise_data = pickle.load(f)
            logger.info("ê¸°ì¡´ Surprise ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        else:
            # ì—†ìœ¼ë©´ CSVì—ì„œ ìƒˆë¡œ ìƒì„±
            logger.info("Surprise ë°ì´í„°ì…‹ ìƒˆë¡œ ìƒì„±")
            self.surprise_data = self.create_surprise_dataset(self.processed_data)
        
        return True
    
    def create_surprise_dataset(self, svd_data):
        """
        Surprise ë¼ì´ë¸ŒëŸ¬ë¦¬ìš© ë°ì´í„°ì…‹ ìƒì„±
        SVD++ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
        """
        logger.info("ğŸ¯ Surprise ë°ì´í„°ì…‹ ìƒì„±")
        
        reader = Reader(rating_scale=(1, 5))
        surprise_data = Dataset.load_from_df(svd_data, reader)
        
        logger.info("âœ… Surprise ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        return surprise_data
    
    def get_stable_performance(self, surprise_data, n_runs=5):
        """
        ì—¬ëŸ¬ seedë¡œ ì‹¤í–‰í•´ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            surprise_data: Surprise ë°ì´í„°ì…‹
            n_runs: ì‹¤í–‰í•  seed ê°œìˆ˜
            
        Returns:
            dict: í‰ê·  ì„±ëŠ¥ê³¼ ë¶„ì‚° ì •ë³´
        """
        logger.info(f"ğŸ¯ ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ ({n_runs}íšŒ ì‹¤í–‰)")
        
        seeds = [42, 123, 456, 789, 999][:n_runs]
        rmse_scores = []
        mae_scores = []
        
        for i, seed in enumerate(seeds, 1):
            logger.info(f"  ì‹¤í–‰ {i}/{n_runs} (seed: {seed})")
            
            # ê° seedë¡œ ëª¨ë¸ ìƒì„±
            model = SVDpp(
                n_factors=50, 
                n_epochs=20, 
                lr_all=0.01,
                reg_all=0.05,
                random_state=seed
            )
            
            # êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •
            cv_results = cross_validate(
                model, surprise_data, 
                measures=['RMSE', 'MAE'], 
                cv=3, 
                verbose=False
            )
            
            rmse_scores.append(cv_results['test_rmse'].mean())
            mae_scores.append(cv_results['test_mae'].mean())
        
        # ê²°ê³¼ ì •ë¦¬
        performance_stats = {
            'rmse_mean': np.mean(rmse_scores),
            'rmse_std': np.std(rmse_scores),
            'rmse_min': np.min(rmse_scores),
            'rmse_max': np.max(rmse_scores),
            'mae_mean': np.mean(mae_scores),
            'mae_std': np.std(mae_scores),
            'mae_min': np.min(mae_scores),
            'mae_max': np.max(mae_scores),
            'individual_rmse': rmse_scores,
            'individual_mae': mae_scores
        }
        
        # ê²°ê³¼ ë¡œê¹…
        logger.info("ğŸ“Š ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ:")
        logger.info(f"  RMSE: {performance_stats['rmse_mean']:.4f} Â± {performance_stats['rmse_std']:.4f}")
        logger.info(f"  RMSE ë²”ìœ„: [{performance_stats['rmse_min']:.4f}, {performance_stats['rmse_max']:.4f}]")
        logger.info(f"  MAE: {performance_stats['mae_mean']:.4f} Â± {performance_stats['mae_std']:.4f}")
        
        return performance_stats

    def optimize_hyperparameters(self, surprise_data, quick_search=True):
        """
        SVD++ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        GridSearchCVë¥¼ í†µí•œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
        """
        logger.info("âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        
        if quick_search:
            # ë¹ ë¥¸ íƒìƒ‰ìš© íŒŒë¼ë¯¸í„°
            param_grid = {
                'n_factors': [50, 100],
                'n_epochs': [20, 30], 
                'lr_all': [0.01, 0.02],
                'reg_all': [0.02, 0.05],
                'random_state': [RANDOM_SEED]
            }
            cv_folds = 3
        else:
            # ì •ë°€ íƒìƒ‰ìš© íŒŒë¼ë¯¸í„°
            param_grid = {
                'n_factors': [50, 100, 150],
                'n_epochs': [20, 30, 40],
                'lr_all': [0.005, 0.01, 0.02],
                'reg_all': [0.02, 0.05, 0.1],
                'random_state': [RANDOM_SEED]
            }
            cv_folds = 5
        
        start_time = time.time()
        gs = GridSearchCV(
            SVDpp,
            param_grid,
            measures=['rmse', 'mae'],
            cv=cv_folds,
            n_jobs=-1,
            joblib_verbose=1
        )
        
        gs.fit(surprise_data)
        end_time = time.time()
        
        self.best_params = gs.best_params['rmse']
        
        logger.info(f"ìµœì í™” ì™„ë£Œ! ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params}")
        logger.info(f"ìµœì  RMSE: {gs.best_score['rmse']:.4f}")
        logger.info(f"ìµœì  MAE: {gs.best_score['mae']:.4f}")
        
        return gs.best_params['rmse'], gs.best_score
    
    def train_final_model(self, surprise_data, best_params):
        """
        ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… SVD++ ëª¨ë¸ êµ¬ì¶•
        """
        logger.info("ğŸ“ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±
        self.best_model = SVDpp(**best_params)
        
        # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
        trainset = surprise_data.build_full_trainset()
        self.best_model.fit(trainset)
        
        logger.info("âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        return self.best_model
    
    def generate_prediction_matrix(self, surprise_data):
        """
        ì „ì²´ ì‚¬ìš©ì-ì‹ë‹¹ ì˜ˆì¸¡ í‰ì  ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        ëª¨ë“  ì‚¬ìš©ìì— ëŒ€í•´ ëª¨ë“  ì‹ë‹¹ì˜ ì˜ˆì¸¡ í‰ì  ê³„ì‚°
        """
        logger.info("ğŸ“Š ì˜ˆì¸¡ í‰ì  ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹œì‘")
        
        # ì „ì²´ trainsetì—ì„œ ì‚¬ìš©ìì™€ ì•„ì´í…œ ID ì¶”ì¶œ
        full_trainset = surprise_data.build_full_trainset()
        
        # ë‚´ë¶€ IDë¥¼ ì›ë³¸ IDë¡œ ë³€í™˜
        unique_users = [full_trainset.to_raw_uid(uid) for uid in full_trainset.all_users()]
        unique_items = [full_trainset.to_raw_iid(iid) for iid in full_trainset.all_items()]
        
        unique_users = sorted(unique_users)
        unique_items = sorted(unique_items)
        
        logger.info(f"ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {len(unique_users)} x {len(unique_items)}")
        
        # ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ ì´ˆê¸°í™”
        prediction_matrix = np.zeros((len(unique_users), len(unique_items)))
        
        start_time = time.time()
        total_predictions = len(unique_users) * len(unique_items)
        
        # ëª¨ë“  ì‚¬ìš©ì-ì•„ì´í…œ ì¡°í•© ì˜ˆì¸¡
        for i, user_id in enumerate(unique_users):
            for j, item_id in enumerate(unique_items):
                pred = self.best_model.predict(user_id, item_id)
                prediction_matrix[i, j] = pred.est
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (i + 1) % 10 == 0:
                progress = ((i + 1) * len(unique_items)) / total_predictions * 100
                logger.info(f"ì˜ˆì¸¡ ì§„í–‰ë¥ : {progress:.1f}%")
        
        end_time = time.time()
        logger.info(f"ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ! ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        self.prediction_matrix = pd.DataFrame(
            prediction_matrix,
            index=unique_users,
            columns=unique_items
        )
        
        return self.prediction_matrix
    
    def save_model_results(self):
        """
        ëª¨ë¸ í•™ìŠµ ê²°ê³¼ë§Œ ì €ì¥
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ê³¼ ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ë§Œ ì €ì¥
        """
        logger.info("ğŸ’¾ ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì‹œì‘")
        
        # 1. ìµœì¢… ëª¨ë¸ ì €ì¥
        if self.best_model is not None:
            model_path = os.path.join(self.result_path, 'svdpp_model.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': self.best_model,
                    'params': self.best_params
                }, f)
            logger.info(f"ëª¨ë¸ ì €ì¥: {model_path}")
        
        # 2. ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥
        if self.prediction_matrix is not None:
            matrix_path = os.path.join(self.result_path, 'prediction_matrix.csv')
            self.prediction_matrix.to_csv(matrix_path)
            logger.info(f"ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥: {matrix_path}")
        
        logger.info("âœ… ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    
    def run_full_pipeline(self, quick_search=True, stability_test=True):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œë¶€í„° ëª¨ë¸ í•™ìŠµ, ê²°ê³¼ ì €ì¥ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ìˆœì°¨ ì‹¤í–‰
        
        Args:
            quick_search: Trueë©´ ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰, Falseë©´ ì •ë°€ íƒìƒ‰
            stability_test: Trueë©´ ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰
        """
        logger.info("ğŸš€ SVD++ ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        
        try:
            # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
            self.load_preprocessed_data()
            
            # 2. ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ (ì„ íƒì‚¬í•­)
            if stability_test:
                stability_results = self.get_stable_performance(self.surprise_data, n_runs=3)
            
                # ì„±ëŠ¥ì´ ë„ˆë¬´ ë¶ˆì•ˆì •í•˜ë©´ ê²½ê³ 
                if stability_results['rmse_std'] > 0.7:
                    logger.warning(f"âš ï¸  RMSE í‘œì¤€í¸ì°¨ê°€ ë†’ìŒ: {stability_results['rmse_std']:.4f}")
                    logger.warning("ë°ì´í„° ì¦ê°€ í›„ ì¬í‰ê°€ ê¶Œì¥")

            # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            best_params, best_scores = self.optimize_hyperparameters(
                self.surprise_data, quick_search=quick_search
            )
            
            # 3. ìµœì¢… ëª¨ë¸ í•™ìŠµ
            self.train_final_model(self.surprise_data, best_params)
            
            # 4. ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            self.generate_prediction_matrix(self.surprise_data)
            
            # 5. ê²°ê³¼ ì €ì¥ (ëª¨ë¸ê³¼ ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ë§Œ)
            self.save_model_results()
            
            logger.info("ğŸ‰ SVD++ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def get_recommendations(self, user_id, n_recommendations=10, exclude_rated=True):
        """
        íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì‹ë‹¹ ì¶”ì²œ
        
        Args:
            user_id: ì‚¬ìš©ì ID
            n_recommendations: ì¶”ì²œí•  ì‹ë‹¹ ìˆ˜
            exclude_rated: ì´ë¯¸ í‰ê°€í•œ ì‹ë‹¹ ì œì™¸ ì—¬ë¶€
        
        Returns:
            ì¶”ì²œ ì‹ë‹¹ ë¦¬ìŠ¤íŠ¸ (ì‹ë‹¹ëª…, ì˜ˆì¸¡í‰ì )
        """
        if self.prediction_matrix is None:
            logger.error("ì˜ˆì¸¡ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        if user_id not in self.prediction_matrix.index:
            logger.warning(f"ì‚¬ìš©ì ID {user_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì‚¬ìš©ìì˜ ì˜ˆì¸¡ í‰ì  ì¶”ì¶œ
        user_predictions = self.prediction_matrix.loc[user_id]
        
        # ì´ë¯¸ í‰ê°€í•œ ì‹ë‹¹ ì œì™¸ (ì„ íƒì‚¬í•­)
        if exclude_rated and self.processed_data is not None:
            rated_restaurants = self.processed_data[
                self.processed_data['userId'] == user_id
            ]['restaurantId'].tolist()
            
            for restaurant_id in rated_restaurants:
                if restaurant_id in user_predictions.index:
                    user_predictions = user_predictions.drop(restaurant_id)
        
        # ìƒìœ„ Nê°œ ì¶”ì²œ
        top_recommendations = user_predictions.nlargest(n_recommendations)
        
        # ì‹ë‹¹ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        recommendations = []
        for restaurant_id, predicted_rating in top_recommendations.items():
            restaurant_name = self.restaurant_mappings['id_to_restaurant'][restaurant_id]
            recommendations.append({
                'restaurant_name': restaurant_name,
                'predicted_rating': round(predicted_rating, 2),
                'restaurant_id': restaurant_id
            })
        
        return recommendations

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import os
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, '..', 'data')  # ìƒìœ„ í´ë”ì˜ data í´ë”
    result_dir = os.path.join(current_dir, '..', 'result')  # ìƒìœ„ í´ë”ì˜ result í´ë”
    
    # ê²°ê³¼ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(result_dir, exist_ok=True)
    
    input_file_path=os.path.join(data_dir, 'rating.xlsx')
    mappings_output_path=os.path.join(result_dir, 'restaurant_mappings.pkl')
    csv_output_path=os.path.join(result_dir, 'svd_data.csv')
    surprise_output_path=os.path.join(result_dir, 'surprise_dataset.pkl')

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ” result í´ë” ì§€ì •)
    pipeline = SVDppRecommendationPipeline(result_dir)
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success = pipeline.run_full_pipeline(quick_search=True, stability_test=True)
    
    if success:
        # ì‚¬ìš©ì 0ì— ëŒ€í•œ ì¶”ì²œ ì˜ˆì‹œ
        recommendations = pipeline.get_recommendations(user_id=0, n_recommendations=10)
        if recommendations:
            print("\nğŸ½ï¸  ì¶”ì²œ ì‹ë‹¹:")
            for rec in recommendations:
                print(f"- {rec['restaurant_name']}: {rec['predicted_rating']}ì ")
