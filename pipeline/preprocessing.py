import pandas as pd
import pickle
import os
import json
import re
from surprise import Dataset, Reader
import logging
from difflib import SequenceMatcher

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedRestaurantDataPreprocessor:
    """ê°œì„ ëœ ì‹ë‹¹ í‰ì  ë°ì´í„° ì „ì²˜ë¦¬ - ì‹¤ì œ ID ê¸°ì¤€ ë§¤í•‘"""
    
    def __init__(self, rating_scale=(1, 5)):
        """
        Args:
            rating_scale: í‰ì  ë²”ìœ„ (ê¸°ë³¸ê°’: 1-5ì )
        """
        self.rating_scale = rating_scale
        self.restaurant_mapping = None
        self.restaurant_reverse_mapping = None
        self.restaurants_df = None
        
    def load_restaurants_data(self, restaurants_csv_path):
        """
        restaurants.csv ë¡œë“œ (í¬ë¡¤ë§ëœ ì‹¤ì œ ì‹ë‹¹ ë°ì´í„°)
        """
        try:
            if not os.path.exists(restaurants_csv_path):
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {restaurants_csv_path}")
                
            self.restaurants_df = pd.read_csv(restaurants_csv_path)
            logger.info(f"ì‹ë‹¹ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.restaurants_df)}ê°œ ì‹ë‹¹")
            
            # ë°ì´í„° ì •ë³´ ì¶œë ¥
            logger.info(f"ì‹ë‹¹ ë°ì´í„° ì»¬ëŸ¼: {list(self.restaurants_df.columns)}")
            logger.info(f"ìƒ˜í”Œ ì‹ë‹¹ëª…: {self.restaurants_df['name'].head(3).tolist()}")
            
            return self.restaurants_df
            
        except Exception as e:
            logger.error(f"ì‹ë‹¹ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def similarity_score(self, str1, str2):
        """
        ë‘ ë¬¸ìì—´ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        """
        return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()
    
    def normalize_restaurant_name(self, name):
        """
        ì‹ë‹¹ëª… ì •ê·œí™” (ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ)
        """
        if pd.isna(name):
            return ""
        
        # 1. ì†Œë¬¸ì ë³€í™˜
        normalized = name.lower()
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì œê±°
        normalized = re.sub(r'[^\wê°€-í£]', '', normalized)
        
        # 3. í”í•œ ì ‘ë¯¸ì‚¬ ì œê±°
        suffixes = ['ë³¸ì ', 'ì§ì˜ì ', 'ì ', 'ì§€ì ', 'ë§¤ì¥', 'ì—­ì ', 'ì í¬']
        for suffix in suffixes:
            if normalized.endswith(suffix):
                normalized = normalized[:-len(suffix)]
                break
        
        return normalized.strip()
    
    def find_matching_restaurant_id(self, survey_name, min_similarity=0.6):
        """
        ì„¤ë¬¸ ì‹ë‹¹ëª…ì— ë§¤ì¹­ë˜ëŠ” restaurants.csvì˜ ì‹¤ì œ ID ì°¾ê¸°
        
        Args:
            survey_name: ì„¤ë¬¸ì—ì„œ ì…ë ¥ë°›ì€ ì‹ë‹¹ëª…
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            int: ë§¤ì¹­ëœ ì‹ë‹¹ì˜ ì‹¤ì œ ID, ë§¤ì¹­ ì‹¤íŒ¨ì‹œ None
        """
        if pd.isna(survey_name) or survey_name.strip() == "":
            return None
        
        survey_normalized = self.normalize_restaurant_name(survey_name)
        
        # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        for _, restaurant_row in self.restaurants_df.iterrows():
            restaurant_normalized = self.normalize_restaurant_name(restaurant_row['name'])
            
            if survey_normalized == restaurant_normalized:
                logger.debug(f"ì •í™• ë§¤ì¹­: '{survey_name}' â†’ '{restaurant_row['name']}' (ID: {restaurant_row['id']})")
                return restaurant_row['id']
        
        # 2ë‹¨ê³„: í¬í•¨ ê´€ê³„ ë§¤ì¹­
        best_match = None
        best_score = 0
        best_restaurant_name = ""
        
        for _, restaurant_row in self.restaurants_df.iterrows():
            restaurant_normalized = self.normalize_restaurant_name(restaurant_row['name'])
            
            # ì„¤ë¬¸ëª…ì´ ì‹ë‹¹ëª…ì— í¬í•¨ë˜ê±°ë‚˜ ê·¸ ë°˜ëŒ€
            if len(survey_normalized) >= 2:
                if survey_normalized in restaurant_normalized:
                    score = len(survey_normalized) / len(restaurant_normalized)
                    if score > best_score:
                        best_score = score
                        best_match = restaurant_row['id']
                        best_restaurant_name = restaurant_row['name']
                
                elif restaurant_normalized in survey_normalized:
                    score = len(restaurant_normalized) / len(survey_normalized)
                    if score > best_score and score > 0.4:  # ìµœì†Œ 40% ì¼ì¹˜
                        best_score = score
                        best_match = restaurant_row['id']
                        best_restaurant_name = restaurant_row['name']
        
        # 3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        if best_match is None:
            for _, restaurant_row in self.restaurants_df.iterrows():
                similarity = self.similarity_score(survey_name, restaurant_row['name'])
                
                if similarity > best_score and similarity >= min_similarity:
                    best_score = similarity
                    best_match = restaurant_row['id']
                    best_restaurant_name = restaurant_row['name']
        
        if best_match:
            logger.debug(f"ìœ ì‚¬ ë§¤ì¹­: '{survey_name}' â†’ '{best_restaurant_name}' (ID: {best_match}, ìœ ì‚¬ë„: {best_score:.3f})")
            return best_match
        
        logger.warning(f"ë§¤ì¹­ ì‹¤íŒ¨: '{survey_name}' - ìœ ì‚¬í•œ ì‹ë‹¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def load_raw_data(self, file_path):
        """
        ì›ë³¸ ì—‘ì…€ íŒŒì¼ì„ ë¡œë“œ
        """
        try:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                
            data = pd.read_excel(file_path)
            logger.info(f"ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {data.shape}")
            return data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def clean_columns(self, data):
        """
        í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
        """
        try:
            # ì œê±°í•  ì»¬ëŸ¼ ì¸ë±ìŠ¤ (íƒ€ì„ìŠ¤íƒ¬í”„, ì¶”ê°€ì…ë ¥ì—¬ë¶€, ê°œì¸ì •ë³´ ê´€ë ¨)
            columns_to_drop = [0, 3, 6, 9, 12, 17, 18]
            cleaned_data = data.drop(data.columns[columns_to_drop], axis=1)
            
            logger.info(f"ì»¬ëŸ¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_data.shape}")
            return cleaned_data
            
        except Exception as e:
            logger.error(f"ì»¬ëŸ¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def extract_restaurant_rating_data(self, data):
        """
        ì‹ë‹¹ ì´ë¦„ê³¼ í‰ì  ë°ì´í„°ë¥¼ ë¶„ë¦¬ ì¶”ì¶œ
        """
        try:
            # ì‹ë‹¹ ì´ë¦„ ì»¬ëŸ¼ (0, 2, 4, 6, 8ë²ˆì§¸)
            restaurant_cols = [0, 2, 4, 6, 8]
            # í‰ì  ì»¬ëŸ¼ (1, 3, 5, 7, 9ë²ˆì§¸)  
            rating_cols = [1, 3, 5, 7, 9]
            
            restaurants = data.iloc[:, restaurant_cols]
            ratings = data.iloc[:, rating_cols]
            
            logger.info("ì‹ë‹¹-í‰ì  ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ")
            return restaurants, ratings
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def convert_to_long_format(self, restaurants, ratings):
        """
        Wide formatì„ Long formatìœ¼ë¡œ ë³€í™˜
        """
        try:
            user_data = []
            
            for user_id in range(len(restaurants)):
                for i in range(5):  # 5ê°œ ì‹ë‹¹
                    restaurant_name = restaurants.iloc[user_id, i]
                    rating_score = ratings.iloc[user_id, i]
                    
                    # ì‹ë‹¹ ì´ë¦„ê³¼ í‰ì ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    if pd.notna(restaurant_name) and pd.notna(rating_score):
                        user_data.append({
                            'user_id': user_id,
                            'restaurant_name': restaurant_name.strip(),
                            'rating': float(rating_score)
                        })
            
            rating_long = pd.DataFrame(user_data)
            logger.info(f"Long format ë³€í™˜ ì™„ë£Œ: {len(rating_long)}ê°œ í‰ì  ë°ì´í„°")
            return rating_long
            
        except Exception as e:
            logger.error(f"Long format ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def create_real_id_mappings(self, rating_long):
        """
        ì‹¤ì œ ID ê¸°ì¤€ ë§¤í•‘ ìƒì„± + ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ê°€ìƒ ID ë¶€ì—¬
        """
        try:
            logger.info("ì‹¤ì œ ID ê¸°ì¤€ ë§¤í•‘ ìƒì„± ì‹œì‘")
            
            if self.restaurants_df is None:
                raise ValueError("restaurants.csvë¥¼ ë¨¼ì € ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤")
            
            # ì„¤ë¬¸ì—ì„œ ë‚˜ì˜¨ ìœ ë‹ˆí¬í•œ ì‹ë‹¹ëª…ë“¤
            unique_survey_restaurants = rating_long['restaurant_name'].unique()
            logger.info(f"ì„¤ë¬¸ì—ì„œ ì¶”ì¶œëœ ì‹ë‹¹ ìˆ˜: {len(unique_survey_restaurants)}ê°œ")
            
            # ì‹¤ì œ ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
            survey_name_to_real_id = {}
            real_id_to_survey_name = {}
            
            # ê°€ìƒ ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (0ë¶€í„° ì‹œì‘í•˜ëŠ” ì–‘ìˆ˜)
            survey_name_to_virtual_id = {}
            virtual_id_to_survey_name = {}
            
            # ë§¤ì¹­ í†µê³„
            exact_matches = 0
            fuzzy_matches = 0
            virtual_id_assigned = 0
            
            # ê°€ìƒ ID ì‹œì‘ê°’ (0ë¶€í„° ì‹œì‘)
            virtual_id_counter = 0
            
            # ê° ì„¤ë¬¸ ì‹ë‹¹ëª…ì— ëŒ€í•´ ì‹¤ì œ ID ì°¾ê¸°
            for survey_name in unique_survey_restaurants:
                real_id = self.find_matching_restaurant_id(survey_name)
                
                if real_id:
                    # ì‹¤ì œ ì‹ë‹¹ê³¼ ë§¤ì¹­ëœ ê²½ìš° - í¬ë¡¤ë§ëœ ì‹¤ì œ ID ì‚¬ìš©
                    survey_name_to_real_id[survey_name] = real_id
                    real_id_to_survey_name[real_id] = survey_name
                    
                    # ë§¤ì¹­ íƒ€ì… í™•ì¸
                    normalized_survey = self.normalize_restaurant_name(survey_name)
                    matched_restaurant = self.restaurants_df[self.restaurants_df['id'] == real_id].iloc[0]
                    normalized_matched = self.normalize_restaurant_name(matched_restaurant['name'])
                    
                    if normalized_survey == normalized_matched:
                        exact_matches += 1
                    else:
                        fuzzy_matches += 1
                else:
                    # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ê°€ìƒ ID ë¶€ì—¬
                    survey_name_to_virtual_id[survey_name] = virtual_id_counter
                    virtual_id_to_survey_name[virtual_id_counter] = survey_name
                    virtual_id_assigned += 1
                    
                    logger.info(f"ê°€ìƒ ID ë¶€ì—¬: '{survey_name}' â†’ ID: {virtual_id_counter}")
                    virtual_id_counter += 1
            
            # ë§¤ì¹­ ê²°ê³¼ í†µê³„
            total_survey_restaurants = len(unique_survey_restaurants)
            total_real_matched = len(survey_name_to_real_id)
            total_processed = total_real_matched + virtual_id_assigned
            
            logger.info(f"ë§¤í•‘ ê²°ê³¼ í†µê³„:")
            logger.info(f"  ì „ì²´ ì„¤ë¬¸ ì‹ë‹¹: {total_survey_restaurants}ê°œ")
            logger.info(f"  ì •í™• ë§¤ì¹­ (ì‹¤ì œ ID): {exact_matches}ê°œ")
            logger.info(f"  ìœ ì‚¬ ë§¤ì¹­ (ì‹¤ì œ ID): {fuzzy_matches}ê°œ")
            logger.info(f"  ê°€ìƒ ID ë¶€ì—¬ (0ë¶€í„°): {virtual_id_assigned}ê°œ")
            logger.info(f"  ì „ì²´ ì²˜ë¦¬ìœ¨: {(total_processed/total_survey_restaurants)*100:.1f}%")
            
            # ì „ì²´ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì‹¤ì œ ID + ê°€ìƒ ID)
            all_survey_to_id = {**survey_name_to_real_id, **survey_name_to_virtual_id}
            all_id_to_survey = {**real_id_to_survey_name, **virtual_id_to_survey_name}
            
            # ëª¨ë“  í‰ì  ë°ì´í„°ì— ID ë§¤í•‘ (ë°ì´í„° ì†ì‹¤ ì—†ìŒ)
            rating_data_with_ids = rating_long.copy()
            rating_data_with_ids['restaurant_id'] = rating_data_with_ids['restaurant_name'].map(all_survey_to_id)
            
            # ë§¤í•‘ ì •ë³´ ì €ì¥ (ì‹¤ì œ IDì™€ ê°€ìƒ ID êµ¬ë¶„í•´ì„œ ì €ì¥)
            self.restaurant_mapping = {
                'real_id_mappings': {
                    'survey_to_real_id': survey_name_to_real_id,
                    'real_id_to_survey': real_id_to_survey_name
                },
                'virtual_id_mappings': {
                    'survey_to_virtual_id': survey_name_to_virtual_id,
                    'virtual_id_to_survey': virtual_id_to_survey_name
                },
                'all_mappings': {
                    'survey_to_id': all_survey_to_id,
                    'id_to_survey': all_id_to_survey
                },
                'statistics': {
                    'total_restaurants': total_survey_restaurants,
                    'real_matched': total_real_matched,
                    'virtual_restaurants': virtual_id_assigned,
                    'exact_matches': exact_matches,
                    'fuzzy_matches': fuzzy_matches,
                    'virtual_id_range': f"0 ~ {virtual_id_counter-1}" if virtual_id_counter > 0 else "ì—†ìŒ"
                }
            }
            
            logger.info(f"ID ë§¤í•‘ ì™„ë£Œ: {len(rating_data_with_ids)}ê°œ í‰ì  ë°ì´í„° ìƒì„± (ë°ì´í„° ì†ì‹¤ ì—†ìŒ)")
            logger.info(f"ì‹¤ì œ ì‹ë‹¹: {len(survey_name_to_real_id)}ê°œ, ê°€ìƒ ì‹ë‹¹: {len(survey_name_to_virtual_id)}ê°œ (ID: 0~{virtual_id_counter-1})")
            
            return rating_data_with_ids
            
        except Exception as e:
            logger.error(f"ID ë§¤í•‘ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def prepare_svd_data(self, rating_data_with_real_ids):
        """
        SVD++ ì…ë ¥ìš© ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ID ì‚¬ìš©)
        """
        try:
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì»¬ëŸ¼ëª… ë³€ê²½
            svd_data = rating_data_with_real_ids[['user_id', 'restaurant_id', 'rating']].copy()
            svd_data.columns = ['userId', 'restaurantId', 'rating']
            
            logger.info(f"SVD++ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {svd_data.shape}")
            logger.info(f"ì‚¬ìš©ì ìˆ˜: {svd_data['userId'].nunique()}ëª…")
            logger.info(f"ì‹ë‹¹ ìˆ˜: {svd_data['restaurantId'].nunique()}ê°œ")
            logger.info(f"í‰ì  ë°ì´í„°: {len(svd_data)}ê°œ")
            
            return svd_data
            
        except Exception as e:
            logger.error(f"SVD++ ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def create_surprise_dataset(self, svd_data):
        """
        Surprise ë¼ì´ë¸ŒëŸ¬ë¦¬ìš© ë°ì´í„°ì…‹ ìƒì„±
        """
        try:
            reader = Reader(rating_scale=self.rating_scale)
            surprise_data = Dataset.load_from_df(svd_data, reader)
            
            logger.info("Surprise ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
            return surprise_data
            
        except Exception as e:
            logger.error(f"Surprise ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def save_mappings(self, output_path):
        """
        ì‹¤ì œ ID + ê°€ìƒ ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        """
        try:
            if self.restaurant_mapping is None:
                raise ValueError("ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            with open(output_path, 'wb') as f:
                pickle.dump(self.restaurant_mapping, f)
                
            logger.info(f"ì „ì²´ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ì €ì¥ ì™„ë£Œ: {output_path}")
            logger.info(f"  - ì‹¤ì œ ID ë§¤í•‘: {len(self.restaurant_mapping['real_id_mappings']['survey_to_real_id'])}ê°œ")
            logger.info(f"  - ê°€ìƒ ID ë§¤í•‘: {len(self.restaurant_mapping['virtual_id_mappings']['survey_to_virtual_id'])}ê°œ")
            
        except Exception as e:
            logger.error(f"ë§¤í•‘ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def save_svd_data_csv(self, svd_data, output_path):
        """
        SVD ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        """
        try:
            svd_data.to_csv(output_path, index=False)
            logger.info(f"SVD ë°ì´í„° CSV ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            logger.error(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def save_surprise_dataset(self, surprise_dataset, output_path):
        """
        Surprise ë°ì´í„°ì…‹ì„ pickle íŒŒì¼ë¡œ ì €ì¥
        """
        try:
            with open(output_path, 'wb') as f:
                pickle.dump(surprise_dataset, f)
            logger.info(f"Surprise ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            logger.error(f"Surprise ë°ì´í„°ì…‹ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def get_data_summary(self, svd_data):
        """
        ë°ì´í„° ìš”ì•½ ì •ë³´ ë°˜í™˜ (ê°€ìƒ IDëŠ” 0ë¶€í„° ì‹œì‘)
        """
        if self.restaurant_mapping is None:
            return {
                'total_ratings': len(svd_data),
                'num_users': svd_data['userId'].nunique(),
                'num_restaurants': svd_data['restaurantId'].nunique(),
                'rating_distribution': svd_data['rating'].value_counts().to_dict(),
                'sparsity': len(svd_data) / (svd_data['userId'].nunique() * svd_data['restaurantId'].nunique())
            }
        
        # ë§¤í•‘ í†µê³„ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stats = self.restaurant_mapping['statistics']
        
        summary = {
            'total_ratings': len(svd_data),
            'num_users': svd_data['userId'].nunique(),
            'num_restaurants': svd_data['restaurantId'].nunique(),
            'real_restaurants': stats['real_matched'],
            'virtual_restaurants': stats['virtual_restaurants'],
            'exact_matches': stats['exact_matches'],
            'fuzzy_matches': stats['fuzzy_matches'],
            'rating_distribution': svd_data['rating'].value_counts().to_dict(),
            'sparsity': len(svd_data) / (svd_data['userId'].nunique() * svd_data['restaurantId'].nunique()),
            'real_match_rate': (stats['real_matched'] / stats['total_restaurants']) * 100 if stats['total_restaurants'] > 0 else 0,
            'virtual_id_range': stats['virtual_id_range']
        }
        return summary
    
    def improved_preprocessing_pipeline(self, rating_file_path, restaurants_csv_path,
                                      mappings_output_path='restaurant_real_id_mappings.pkl', 
                                      csv_output_path='svd_data.csv', 
                                      surprise_output_path='surprise_dataset.pkl', 
                                      save_csv=True, save_surprise=True):
        """
        ê°œì„ ëœ ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹¤ì œ ID ê¸°ì¤€)
        """
        try:
            logger.info("ğŸš€ ê°œì„ ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì‹¤ì œ ID ê¸°ì¤€)")
            
            # 1. ì‹ë‹¹ ë°ì´í„° ë¡œë“œ (restaurants.csv)
            self.load_restaurants_data(restaurants_csv_path)
            
            # 2. ì›ë³¸ ì„¤ë¬¸ ë°ì´í„° ë¡œë“œ
            raw_data = self.load_raw_data(rating_file_path)
            
            # 3. ì»¬ëŸ¼ ì •ë¦¬
            cleaned_data = self.clean_columns(raw_data)
            
            # 4. ì‹ë‹¹-í‰ì  ë°ì´í„° ë¶„ë¦¬
            restaurants, ratings = self.extract_restaurant_rating_data(cleaned_data)
            
            # 5. Long format ë³€í™˜
            rating_long = self.convert_to_long_format(restaurants, ratings)
            
            # 6. ì‹¤ì œ ID ê¸°ì¤€ ë§¤í•‘ ìƒì„± (í•µì‹¬ ê°œì„ !)
            rating_data_with_real_ids = self.create_real_id_mappings(rating_long)
            
            # 7. SVD++ ë°ì´í„° ì¤€ë¹„
            svd_data = self.prepare_svd_data(rating_data_with_real_ids)
            
            # 8. Surprise ë°ì´í„°ì…‹ ìƒì„±
            surprise_dataset = self.create_surprise_dataset(svd_data)
            
            # 9. ì‹¤ì œ ID ë§¤í•‘ ì €ì¥
            self.save_mappings(mappings_output_path)
            
            # 10. SVD ë°ì´í„° CSV ì €ì¥
            if save_csv:
                self.save_svd_data_csv(svd_data, csv_output_path)
            
            # 11. Surprise ë°ì´í„°ì…‹ ì €ì¥
            if save_surprise:
                self.save_surprise_dataset(surprise_dataset, surprise_output_path)
            
            # 12. ë°ì´í„° ìš”ì•½
            data_summary = self.get_data_summary(svd_data)

            logger.info("âœ… ê°œì„ ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
            logger.info(f"  - ì‹¤ì œ ì‹ë‹¹: {data_summary['real_restaurants']}ê°œ")
            logger.info(f"  - ê°€ìƒ ì‹ë‹¹: {data_summary['virtual_restaurants']}ê°œ")
            logger.info(f"  - ì „ì²´ ì‹ë‹¹: {data_summary['num_restaurants']}ê°œ")
            logger.info(f"  - ì‚¬ìš©ì: {data_summary['num_users']}ëª…")
            logger.info(f"  - í‰ì  ë°ì´í„°: {data_summary['total_ratings']}ê°œ")
            logger.info(f"  - ì‹¤ì œ ì‹ë‹¹ ë§¤ì¹­ë¥ : {data_summary['real_match_rate']:.1f}%")
            logger.info(f"  - ê°€ìƒ ID ë²”ìœ„: {data_summary['virtual_id_range']}")
            logger.info(f"  - ë°ì´í„° ë°€ë„: {data_summary['sparsity']:.4f}")

            return svd_data, surprise_dataset, data_summary
        
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
            

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import os
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, '..', 'data')
    result_dir = os.path.join(current_dir, '..', 'result')
    
    # ê²°ê³¼ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(result_dir, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ
    rating_file_path = os.path.join(data_dir, 'rating.xlsx')
    restaurants_csv_path = os.path.join(data_dir, 'restaurants.csv')
    mappings_output_path = os.path.join(result_dir, 'restaurant_real_id_mappings.pkl')
    csv_output_path = os.path.join(result_dir, 'svd_data.csv')
    surprise_output_path = os.path.join(result_dir, 'surprise_dataset.pkl')

    # ê°œì„ ëœ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = ImprovedRestaurantDataPreprocessor(rating_scale=(1, 5))
    
    try:
        print("=== ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ===")
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        svd_data, surprise_dataset, summary = preprocessor.improved_preprocessing_pipeline(
            rating_file_path=rating_file_path,
            restaurants_csv_path=restaurants_csv_path,
            mappings_output_path=mappings_output_path,
            csv_output_path=csv_output_path,
            surprise_output_path=surprise_output_path,
            save_csv=True,
            save_surprise=True
        )
        
        print("ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ˆ ì‹¤ì œ ì‹ë‹¹ ë§¤ì¹­ë¥ : {summary['real_match_rate']:.1f}%")
        print(f"ğŸ“Š ì‹¤ì œ ì‹ë‹¹: {summary['real_restaurants']}ê°œ")
        print(f"ğŸ“Š ê°€ìƒ ì‹ë‹¹: {summary['virtual_restaurants']}ê°œ")
        print(f"ğŸ“Š ê°€ìƒ ID ë²”ìœ„: {summary['virtual_id_range']}")
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {summary}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print(f"\nğŸ” ìƒì„±ëœ SVD ë°ì´í„° ìƒ˜í”Œ:")
        print(svd_data.head())
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")