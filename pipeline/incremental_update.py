import pandas as pd
import json
import os
import logging
import pickle
from datetime import datetime
from .preprocessing import ImprovedRestaurantDataPreprocessor
from .svdpp import SVDppRecommendationPipeline
from .contentRecommender import ContentBasedRecommender

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IncrementalUpdatePipeline:
    """
    ì£¼ê¸°ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì¦ë¶„ í•™ìŠµ íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, base_dir='.'):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, 'data')
        self.result_dir = os.path.join(base_dir, 'result')
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir = os.path.join(base_dir, 'backup')
        os.makedirs(self.backup_dir, exist_ok=True)
        
    def load_feedback_data(self):
        """
        í”¼ë“œë°± íì—ì„œ ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ
        """
        feedback_file = os.path.join(self.data_dir, 'feedback_queue.jsonl')
        
        if not os.path.exists(feedback_file):
            logger.info("ìˆ˜ì§‘ëœ í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        feedback_data = []
        with open(feedback_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feedback_data.append(json.loads(line.strip()))
        
        logger.info(f"í”¼ë“œë°± ë°ì´í„° {len(feedback_data)}ê°œ ë¡œë“œ ì™„ë£Œ")
        return feedback_data
    
    def process_new_ratings(self, feedback_data):
        """
        í”¼ë“œë°± ë°ì´í„°ì—ì„œ ìƒˆë¡œìš´ í‰ì  ë°ì´í„° ì¶”ì¶œ ë° ê¸°ì¡´ ë°ì´í„°ì™€ í†µí•©
        """
        if not feedback_data:
            logger.info("ìƒˆë¡œìš´ í‰ì  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ê¸°ì¡´ SVD ë°ì´í„° ë¡œë“œ
        svd_data_path = os.path.join(self.result_dir, 'svd_data.csv')
        existing_svd_data = pd.read_csv(svd_data_path)
        
        # ìƒˆë¡œìš´ í‰ì  ë°ì´í„° ì¤€ë¹„
        new_ratings = []
        new_restaurants = []
        
        for feedback in feedback_data:
            # í‰ì  ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            if 'rating' in feedback and feedback['rating'] is not None:
                new_ratings.append({
                    'userId': int(feedback['userId']),
                    'restaurantId': int(feedback['restaurantId']),
                    'rating': float(feedback['rating'])
                })
            
            # ìƒˆë¡œìš´ ì‹ë‹¹ ì •ë³´ê°€ ìˆìœ¼ë©´ ìˆ˜ì§‘
            if 'restaurantCategory' in feedback and 'menuAverage' in feedback:
                new_restaurants.append({
                    'id': int(feedback['restaurantId']),
                    'category': feedback['restaurantCategory'],
                    'menu_average': float(feedback['menuAverage'])
                })
        
        logger.info(f"ìƒˆë¡œìš´ í‰ì : {len(new_ratings)}ê°œ, ìƒˆë¡œìš´ ì‹ë‹¹: {len(new_restaurants)}ê°œ")
        
        if new_ratings:
            # ìƒˆë¡œìš´ í‰ì ì„ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
            new_ratings_df = pd.DataFrame(new_ratings)
            updated_svd_data = pd.concat([existing_svd_data, new_ratings_df], ignore_index=True)
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ì‚¬ìš©ìê°€ ê°™ì€ ì‹ë‹¹ì— ì—¬ëŸ¬ í‰ì ì„ ì¤€ ê²½ìš° ìµœì‹  ê²ƒë§Œ ìœ ì§€)
            updated_svd_data = updated_svd_data.drop_duplicates(
                subset=['userId', 'restaurantId'], 
                keep='last'
            )
            
            # ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì €ì¥
            updated_svd_data.to_csv(svd_data_path, index=False)
            logger.info(f"SVD ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(updated_svd_data)}ê°œ í‰ì ")
            
            return True
        
        return False
    
    def process_new_restaurants(self, feedback_data):
        """
        ìƒˆë¡œìš´ ì‹ë‹¹ ì •ë³´ë¥¼ restaurants.csvì— ì¶”ê°€
        """
        restaurants_path = os.path.join(self.data_dir, 'restaurants.csv')
        existing_restaurants = pd.read_csv(restaurants_path)
        
        new_restaurants = []
        for feedback in feedback_data:
            if ('restaurantId' in feedback and 'restaurantCategory' in feedback 
                and 'menuAverage' in feedback):
                
                restaurant_id = int(feedback['restaurantId'])
                
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ì¸ì§€ í™•ì¸
                if restaurant_id not in existing_restaurants['id'].values:
                    new_restaurants.append({
                        'id': restaurant_id,
                        'name': feedback.get('restaurantName', f'Restaurant_{restaurant_id}'),
                        'category': feedback['restaurantCategory'],
                        'menu_average': float(feedback['menuAverage'])
                    })
        
        if new_restaurants:
            new_restaurants_df = pd.DataFrame(new_restaurants)
            updated_restaurants = pd.concat([existing_restaurants, new_restaurants_df], ignore_index=True)
            
            # ì¤‘ë³µ ì œê±°
            updated_restaurants = updated_restaurants.drop_duplicates(subset=['id'], keep='last')
            updated_restaurants.to_csv(restaurants_path, index=False)
            
            logger.info(f"ìƒˆë¡œìš´ ì‹ë‹¹ {len(new_restaurants)}ê°œ ì¶”ê°€")
            return True
        
        return False
    
    def backup_current_models(self):
        """
        í˜„ì¬ ëª¨ë¸ë“¤ì„ ë°±ì—…
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        files_to_backup = [
            'svdpp_model.pkl',
            'prediction_matrix.csv',
            'content_based_matrix.csv'
        ]
        
        for filename in files_to_backup:
            source_path = os.path.join(self.result_dir, filename)
            if os.path.exists(source_path):
                backup_path = os.path.join(self.backup_dir, f'{timestamp}_{filename}')
                import shutil
                shutil.copy2(source_path, backup_path)
                logger.info(f"ë°±ì—… ì™„ë£Œ: {filename}")
    
    def retrain_models(self):
        """
        ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ
        """
        logger.info("ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")
        
        try:
            # SVD++ ëª¨ë¸ ì¬í•™ìŠµ
            svd_pipeline = SVDppRecommendationPipeline(self.result_dir)
            success = svd_pipeline.run_full_pipeline(quick_search=True, stability_test=False)
            
            if not success:
                logger.error("SVD++ ì¬í•™ìŠµ ì‹¤íŒ¨")
                return False
            
            # ì½˜í…ì¸  ê¸°ë°˜ ë§¤íŠ¸ë¦­ìŠ¤ ì¬ìƒì„±
            restaurants_path = os.path.join(self.data_dir, 'restaurants.csv')
            content_features_path = os.path.join(self.data_dir, 'content_features.csv')
            mappings_path = os.path.join(self.result_dir, 'restaurant_real_id_mappings.pkl')
            svd_data_path = os.path.join(self.result_dir, 'svd_data.csv')
            
            if os.path.exists(content_features_path):
                content_recommender = ContentBasedRecommender(
                    restaurants_path, content_features_path, mappings_path
                )
                content_recommender.build_similarity_matrix()
                
                svd_data = pd.read_csv(svd_data_path)
                content_matrix = content_recommender.generate_user_restaurant_matrix(svd_data)
                
                content_matrix_path = os.path.join(self.result_dir, 'content_based_matrix.csv')
                content_matrix.to_csv(content_matrix_path)
                
                logger.info("ì½˜í…ì¸  ê¸°ë°˜ ë§¤íŠ¸ë¦­ìŠ¤ ì¬ìƒì„± ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì¬í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def archive_processed_feedback(self):
        """
        ì²˜ë¦¬ëœ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì•„ì¹´ì´ë¸Œí•˜ê³  í íŒŒì¼ ì´ˆê¸°í™”
        """
        feedback_file = os.path.join(self.data_dir, 'feedback_queue.jsonl')
        
        if os.path.exists(feedback_file):
            # ì•„ì¹´ì´ë¸Œ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            archive_file = os.path.join(self.data_dir, f'processed_feedback_{timestamp}.jsonl')
            
            # íŒŒì¼ ì´ë™
            import shutil
            shutil.move(feedback_file, archive_file)
            
            logger.info(f"í”¼ë“œë°± ë°ì´í„° ì•„ì¹´ì´ë¸Œ: {archive_file}")
    
    def update_last_update_time(self):
        """
        ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ë¡
        """
        update_file = os.path.join(self.result_dir, 'last_update.txt')
        with open(update_file, 'w') as f:
            f.write(datetime.now().isoformat())
    
    def run_incremental_update(self):
        """
        ì „ì²´ ì¦ë¶„ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        """
        logger.info("ğŸ”„ ì¦ë¶„ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        try:
            # 1. í”¼ë“œë°± ë°ì´í„° ë¡œë“œ
            feedback_data = self.load_feedback_data()
            
            if not feedback_data:
                logger.info("ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return True
            
            # 2. í˜„ì¬ ëª¨ë¸ ë°±ì—…
            self.backup_current_models()
            
            # 3. ìƒˆë¡œìš´ í‰ì  ë°ì´í„° ì²˜ë¦¬
            ratings_updated = self.process_new_ratings(feedback_data)
            
            # 4. ìƒˆë¡œìš´ ì‹ë‹¹ ì •ë³´ ì²˜ë¦¬
            restaurants_updated = self.process_new_restaurants(feedback_data)
            
            # 5. ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë©´ ëª¨ë¸ ì¬í•™ìŠµ
            if ratings_updated or restaurants_updated:
                retrain_success = self.retrain_models()
                
                if not retrain_success:
                    logger.error("ëª¨ë¸ ì¬í•™ìŠµ ì‹¤íŒ¨")
                    return False
            
            # 6. ì²˜ë¦¬ëœ í”¼ë“œë°± ì•„ì¹´ì´ë¸Œ
            self.archive_processed_feedback()
            
            # 7. ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ë¡
            self.update_last_update_time()
            
            logger.info("âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False


# ìˆ˜ë™ ì‹¤í–‰ì„ ìœ„í•œ í•¨ìˆ˜
def run_manual_update():
    """
    ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤í–‰
    """
    pipeline = IncrementalUpdatePipeline()
    return pipeline.run_incremental_update()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = run_manual_update()
    if success:
        print("âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì„±ê³µ")
    else:
        print("âŒ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")